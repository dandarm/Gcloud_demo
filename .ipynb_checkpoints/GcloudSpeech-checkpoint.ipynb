{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import speech_v1p1beta1\n",
    "from google.cloud.speech_v1p1beta1 import enums\n",
    "\n",
    "import io\n",
    "\n",
    "## not working\n",
    "#from google.cloud import speech_v1\n",
    "#from google.cloud.speech_v1 import enums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/daniele/Documenti/Progetti/ASR/GoogleCloudSpeech/credentials.json\n"
     ]
    }
   ],
   "source": [
    "# Questa variabile deve contenere il path del file di credenziali\n",
    "!echo \"$GOOGLE_APPLICATION_CREDENTIALS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_params = {\n",
    "    \"language_code\": \"it-IT\", #\"en-US\"\n",
    "    \"sample_rate_hertz\": 16000, #44100\n",
    "    \"encoding\": enums.RecognitionConfig.AudioEncoding.MP3,\n",
    "    #\"encoding\": enums.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_recognize(audiofile_path, config, local=True):\n",
    "    \"\"\"\n",
    "    Performs synchronous speech recognition on an audio file\n",
    "\n",
    "    Arguments:\n",
    "      storage_uri URI for audio file in Cloud Storage, e.g. gs://[BUCKET]/[FILE]\n",
    "      or\n",
    "      local_file_path Path to local audio file, e.g. /path/audio.wav\n",
    "    \"\"\"\n",
    "\n",
    "    client = speech_v1p1beta1.SpeechClient()\n",
    "\n",
    "    if(local):\n",
    "        with io.open(audiofile_path, \"rb\") as f:\n",
    "            content = f.read()\n",
    "        audio = {\"content\": content}\n",
    "    else:\n",
    "        # storage_uri = 'gs://cloud-samples-data/speech/brooklyn_bridge.mp3'\n",
    "        storage_uri = audiofile_path\n",
    "        audio = {\"uri\": storage_uri}\n",
    "    \n",
    "    response = client.recognize(config, audio)\n",
    "    for result in response.results:\n",
    "        # First alternative is the most probable result\n",
    "        alternative = result.alternatives[0]\n",
    "        print(u\"Transcript: {}\".format(alternative.transcript))\n",
    "        \n",
    "    return alternative.transcript"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def main():\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--storage_uri\",\n",
    "        type=str,\n",
    "        default=\"gs://cloud-samples-data/speech/brooklyn_bridge.mp3\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    sample_recognize(args.storage_uri)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript: aneddoti barzellette episodi\n"
     ]
    }
   ],
   "source": [
    "filepath = 'parte1.mp3'\n",
    "sample_recognize(filepath, config_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript: è una delle mie fobie quando prendo la macchina ho paura di non trovare parcheggio in centro\n"
     ]
    }
   ],
   "source": [
    "filepath = 'parte2.mp3'\n",
    "sample_recognize(filepath, config_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript: è una azione intenzionale Noi guardiamo di prof\n"
     ]
    }
   ],
   "source": [
    "filepath = 'parte3.mp3'\n",
    "sample_recognize(filepath, config_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript: l'abbacchi e le galline perché so senza spine non so come baccalà\n"
     ]
    }
   ],
   "source": [
    "filepath = 'Bri1.mp3'\n",
    "sample_recognize(filepath, config_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript: spaghetti amatriciana\n"
     ]
    }
   ],
   "source": [
    "filepath = 'Bri2.mp3'\n",
    "sample_recognize(filepath, config_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript: Sì la socera more se famo du spaghetti amatriciana\n"
     ]
    }
   ],
   "source": [
    "filepath = 'Bri2_2.mp3'\n",
    "sample_recognize(filepath, config_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_preds = 'lasila sofceranuooriese famo duspa ghettiamadricanaf'\n",
    "decoded_targets = 'Sì la socera more se famo du spaghetti amatriciana'\n",
    "test_cer, test_wer = [], []\n",
    "#for j in range(len(decoded_preds)):\n",
    "    #test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
    "    #test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
    "cer(decoded_targets, decoded_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test su mozilla dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _levenshtein_distance(ref, hyp):\n",
    "    \"\"\"Levenshtein distance is a string metric for measuring the difference\n",
    "    between two sequences. Informally, the levenshtein disctance is defined as\n",
    "    the minimum number of single-character edits (substitutions, insertions or\n",
    "    deletions) required to change one word into the other. We can naturally\n",
    "    extend the edits to word level when calculate levenshtein disctance for\n",
    "    two sentences.\n",
    "    \"\"\"\n",
    "    m = len(ref)\n",
    "    n = len(hyp)\n",
    "\n",
    "    # special case\n",
    "    if ref == hyp:\n",
    "        return 0\n",
    "    if m == 0:\n",
    "        return n\n",
    "    if n == 0:\n",
    "        return m\n",
    "\n",
    "    if m < n:\n",
    "        ref, hyp = hyp, ref\n",
    "        m, n = n, m\n",
    "\n",
    "    # use O(min(m, n)) space\n",
    "    distance = np.zeros((2, n + 1), dtype=np.int32)\n",
    "\n",
    "    # initialize distance matrix\n",
    "    for j in range(0,n + 1):\n",
    "        distance[0][j] = j\n",
    "\n",
    "    # calculate levenshtein distance\n",
    "    for i in range(1, m + 1):\n",
    "        prev_row_idx = (i - 1) % 2\n",
    "        cur_row_idx = i % 2\n",
    "        distance[cur_row_idx][0] = i\n",
    "        for j in range(1, n + 1):\n",
    "            if ref[i - 1] == hyp[j - 1]:\n",
    "                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n",
    "            else:\n",
    "                s_num = distance[prev_row_idx][j - 1] + 1\n",
    "                i_num = distance[cur_row_idx][j - 1] + 1\n",
    "                d_num = distance[prev_row_idx][j] + 1\n",
    "                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n",
    "\n",
    "    return distance[m % 2][n]\n",
    "\n",
    "def word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n",
    "    \"\"\"Compute the levenshtein distance between reference sequence and\n",
    "    hypothesis sequence in word-level.\n",
    "    :param reference: The reference sentence.\n",
    "    :type reference: basestring\n",
    "    :param hypothesis: The hypothesis sentence.\n",
    "    :type hypothesis: basestring\n",
    "    :param ignore_case: Whether case-sensitive or not.\n",
    "    :type ignore_case: bool\n",
    "    :param delimiter: Delimiter of input sentences.\n",
    "    :type delimiter: char\n",
    "    :return: Levenshtein distance and word number of reference sentence.\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    if ignore_case == True:\n",
    "        reference = reference.lower()\n",
    "        hypothesis = hypothesis.lower()\n",
    "\n",
    "    ref_words = reference.split(delimiter)\n",
    "    hyp_words = hypothesis.split(delimiter)\n",
    "\n",
    "    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n",
    "    return float(edit_distance), len(ref_words)\n",
    "\n",
    "def char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\n",
    "    \"\"\"Compute the levenshtein distance between reference sequence and\n",
    "    hypothesis sequence in char-level.\n",
    "    :param reference: The reference sentence.\n",
    "    :type reference: basestring\n",
    "    :param hypothesis: The hypothesis sentence.\n",
    "    :type hypothesis: basestring\n",
    "    :param ignore_case: Whether case-sensitive or not.\n",
    "    :type ignore_case: bool\n",
    "    :param remove_space: Whether remove internal space characters\n",
    "    :type remove_space: bool\n",
    "    :return: Levenshtein distance and length of reference sentence.\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    if ignore_case == True:\n",
    "        reference = reference.lower()\n",
    "        hypothesis = hypothesis.lower()\n",
    "\n",
    "    join_char = ' '\n",
    "    if remove_space == True:\n",
    "        join_char = ''\n",
    "\n",
    "    reference = join_char.join(filter(None, reference.split(' ')))\n",
    "    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n",
    "\n",
    "    edit_distance = _levenshtein_distance(reference, hypothesis)\n",
    "    return float(edit_distance), len(reference)\n",
    "\n",
    "def wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n",
    "    \"\"\"Calculate word error rate (WER). WER compares reference text and\n",
    "    hypothesis text in word-level. WER is defined as:\n",
    "    .. math::\n",
    "        WER = (Sw + Dw + Iw) / Nw\n",
    "    where\n",
    "    .. code-block:: text\n",
    "        Sw is the number of words subsituted,\n",
    "        Dw is the number of words deleted,\n",
    "        Iw is the number of words inserted,\n",
    "        Nw is the number of words in the reference\n",
    "    We can use levenshtein distance to calculate WER. Please draw an attention\n",
    "    that empty items will be removed when splitting sentences by delimiter.\n",
    "    :param reference: The reference sentence.\n",
    "    :type reference: basestring\n",
    "    :param hypothesis: The hypothesis sentence.\n",
    "    :type hypothesis: basestring\n",
    "    :param ignore_case: Whether case-sensitive or not.\n",
    "    :type ignore_case: bool\n",
    "    :param delimiter: Delimiter of input sentences.\n",
    "    :type delimiter: char\n",
    "    :return: Word error rate.\n",
    "    :rtype: float\n",
    "    :raises ValueError: If word number of reference is zero.\n",
    "    \"\"\"\n",
    "    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case,\n",
    "                                         delimiter)\n",
    "\n",
    "    if ref_len == 0:\n",
    "        raise ValueError(\"Reference's word number should be greater than 0.\")\n",
    "\n",
    "    wer = float(edit_distance) / ref_len\n",
    "    return wer\n",
    "\n",
    "\n",
    "def cer(reference, hypothesis, ignore_case=False, remove_space=False):\n",
    "    \"\"\"Calculate charactor error rate (CER). CER compares reference text and\n",
    "    hypothesis text in char-level. CER is defined as:\n",
    "    .. math::\n",
    "        CER = (Sc + Dc + Ic) / Nc\n",
    "    where\n",
    "    .. code-block:: text\n",
    "        Sc is the number of characters substituted,\n",
    "        Dc is the number of characters deleted,\n",
    "        Ic is the number of characters inserted\n",
    "        Nc is the number of characters in the reference\n",
    "    We can use levenshtein distance to calculate CER. Chinese input should be\n",
    "    encoded to unicode. Please draw an attention that the leading and tailing\n",
    "    space characters will be truncated and multiple consecutive space\n",
    "    characters in a sentence will be replaced by one space character.\n",
    "    :param reference: The reference sentence.\n",
    "    :type reference: basestring\n",
    "    :param hypothesis: The hypothesis sentence.\n",
    "    :type hypothesis: basestring\n",
    "    :param ignore_case: Whether case-sensitive or not.\n",
    "    :type ignore_case: bool\n",
    "    :param remove_space: Whether remove internal space characters\n",
    "    :type remove_space: bool\n",
    "    :return: Character error rate.\n",
    "    :rtype: float\n",
    "    :raises ValueError: If the reference length is zero.\n",
    "    \"\"\"\n",
    "    edit_distance, ref_len = char_errors(reference, hypothesis, ignore_case,\n",
    "                                         remove_space)\n",
    "\n",
    "    if ref_len == 0:\n",
    "        raise ValueError(\"Length of reference should be greater than 0.\")\n",
    "\n",
    "    cer = float(edit_distance) / ref_len\n",
    "    return cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "path_data = Path(\"../voice.mozilla.org_it_datasets/it\")\n",
    "test_dir = path_data / \"test_mozillaData\"\n",
    "clips_dir = test_dir / 'clips'\n",
    "labels_dir = test_dir / 'labels'\n",
    "\n",
    "import sys\n",
    "sys.path.append('../pytorch_ASR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decode with google ASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriptions = {}\n",
    "audiofiles = clips_dir.glob('*.mp3')\n",
    "for i,a in enumerate(audiofiles):\n",
    "    t = sample_recognize(a, config_params)\n",
    "    transcriptions[a.name.replace('.mp3','')] = t\n",
    "    print(str(i) + \"...\" + t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "a_file = open(\"transcriptions_google.pkl\", \"wb\")\n",
    "pickle.dump(transcriptions, a_file)\n",
    "a_file.close()\n",
    "\n",
    "#a_file = open(\"transcriptions_google.pkl\", \"rb\")\n",
    "#transcriptions = pickle. load(a_file)\n",
    "#print(output)\n",
    "#a_file. close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load labels from Mozilla dataset and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelfiles = labels_dir.glob('*.txt')\n",
    "labels = {}\n",
    "for i,l in enumerate(labelfiles):\n",
    "    file = open(l, \"r\") \n",
    "    labels[l.name.replace('.txt','')] = file.read() \n",
    "    file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "preds = []\n",
    "for k in transcriptions.keys():\n",
    "    targets.append(labels[k])\n",
    "    preds.append(transcriptions[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character error rate: 0.093\n",
      "Word error rate: 0.258\n"
     ]
    }
   ],
   "source": [
    "test_cer, test_wer = [], []\n",
    "for k in range(len(targets)):\n",
    "    test_cer.append(cer(preds[k], targets[k]))\n",
    "    test_wer.append(wer(preds[k], targets[k]))\n",
    "    \n",
    "avg_cer = sum(test_cer)/len(test_cer)\n",
    "avg_wer = sum(test_wer)/len(test_wer)\n",
    "print(f'Character error rate: {avg_cer:.3f}')\n",
    "print(f'Word error rate: {avg_wer:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
